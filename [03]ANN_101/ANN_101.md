# 신경망 101 - 근사함수를 찾자

태그: UniversalApproximationTheorem
No.: 3

### 인공지능(Artificial Intelligence)란 무엇인가?

- Everything is (nothing but) a Function
    - Text generation: ex) translation: “hello” maps to “안녕하세요” or “안녕”
        - 함수이지만 hallucinationn 값이 들어가서 함수가 아닌것처럼 보일 수 있음. 하지만 창의적인 답변을 위해선 필요함
    - Image classification: ex) maps to ‘dog’
    - Image segmentation” ex) 누끼 따기, 배경 날리기

![Untitled](%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC%20101%20-%20%E1%84%80%E1%85%B3%E1%86%AB%E1%84%89%E1%85%A1%E1%84%92%E1%85%A1%E1%86%B7%E1%84%89%E1%85%AE%E1%84%85%E1%85%B3%E1%86%AF%20%E1%84%8E%E1%85%A1%E1%86%BD%E1%84%8C%E1%85%A1%2000b35dc3fdb5455dab940988d3c5c878/Untitled.png)

- 위 두가지 종류의 인공지능은 사용하는 목적도 다르고, 데이터의 형태도 다르다.
    - 요즘은 Transformer 기반이 제일 대세 + Diffusion 계열
- 인간의 판단력을 모사하는 인공지능
    - 정확한 답 하나가 요구됨

### Approximation

- 어떤 함수를 만들어야 할 지 결정했다면? 정확한 함수를 만들수 없으니까 근사함수를 만든다.
- to make an approximation of f, we need: (in Deep Learning)
    - 1) a method for evaluation,
        - test data set
        - metric
    - 2) candidate models, and
        - ANN or its variation (CNN, RNN, Transformer, …)
            - model hyperparameters: 다 돌릴 수는 없으니까 한계를 설정
                - ex) width, height…
            - model parameters
    - 3) a way to find the model parameters.
        - train data set (to update parameters)
            - validation data set (to update hyperparameters)
        - real valued differentiable cost function
            - MSE써도 됨, 수렴을 잘 하기 위한 term들,
        - grandient descent algorithm (to update model parameters)
        

### Artificial Neural Network

![Untitled](%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC%20101%20-%20%E1%84%80%E1%85%B3%E1%86%AB%E1%84%89%E1%85%A1%E1%84%92%E1%85%A1%E1%86%B7%E1%84%89%E1%85%AE%E1%84%85%E1%85%B3%E1%86%AF%20%E1%84%8E%E1%85%A1%E1%86%BD%E1%84%8C%E1%85%A1%2000b35dc3fdb5455dab940988d3c5c878/Untitled%201.png)

- Each circle represents a real  number (neuron)
    - green: 입력층, blue: 숨겨진층, yello: 출력층
    - 층마다 linear layer라고 부르기도 함
- i = 2, j = 5까지 가능
- activation function: 연속 함수 ex) RELU, sigmoid

### 손으로 쓰면서 이해해보자!

![SmartSelect_20231106_100805_Samsung Notes.jpg](%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC%20101%20-%20%E1%84%80%E1%85%B3%E1%86%AB%E1%84%89%E1%85%A1%E1%84%92%E1%85%A1%E1%86%B7%E1%84%89%E1%85%AE%E1%84%85%E1%85%B3%E1%86%AF%20%E1%84%8E%E1%85%A1%E1%86%BD%E1%84%8C%E1%85%A1%2000b35dc3fdb5455dab940988d3c5c878/SmartSelect_20231106_100805_Samsung_Notes.jpg)

![Untitled](%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC%20101%20-%20%E1%84%80%E1%85%B3%E1%86%AB%E1%84%89%E1%85%A1%E1%84%92%E1%85%A1%E1%86%B7%E1%84%89%E1%85%AE%E1%84%85%E1%85%B3%E1%86%AF%20%E1%84%8E%E1%85%A1%E1%86%BD%E1%84%8C%E1%85%A1%2000b35dc3fdb5455dab940988d3c5c878/Untitled%202.png)

- 어떤 임의의 함수가 있으면 이걸 확대 이동 시키는걸 반복시키면 내가 목표로 하는 함수(모델)에 근사한 함수를 만든다!
    - hidden($\sigma$)이 n개가 되면 근사할 수 있음.
    - ex) Step function
- 모델 학습이란?
    - 손실 함수가 0에 가까워지도록 parameters(a11, a21, a12, a22, b11, b21, b12, b22)를 설정하는 것.

### Affine Maps

![Untitled](%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC%20101%20-%20%E1%84%80%E1%85%B3%E1%86%AB%E1%84%89%E1%85%A1%E1%84%92%E1%85%A1%E1%86%B7%E1%84%89%E1%85%AE%E1%84%85%E1%85%B3%E1%86%AF%20%E1%84%8E%E1%85%A1%E1%86%BD%E1%84%8C%E1%85%A1%2000b35dc3fdb5455dab940988d3c5c878/Untitled%203.png)

- 정사각형 → 평행사변형으로 바꿔주기도 함.
- x를 Ax+b (A=weight, b=bias) 로 바꿔주는 것

### Component-wise Composition

- $\sigma(y_1)$: 벡터의 원소별로 옮겨주는 것

![Untitled](%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC%20101%20-%20%E1%84%80%E1%85%B3%E1%86%AB%E1%84%89%E1%85%A1%E1%84%92%E1%85%A1%E1%86%B7%E1%84%89%E1%85%AE%E1%84%85%E1%85%B3%E1%86%AF%20%E1%84%8E%E1%85%A1%E1%86%BD%E1%84%8C%E1%85%A1%2000b35dc3fdb5455dab940988d3c5c878/Untitled%204.png)

### Universal Approximation Theorem

- $\sigma$(연속함수)가 polynomial이 아니면, target function f 에 대해서, compact subset K (a closed interval)를 넣었을 때 오차를 원하는 만큼 작게 줄일 수 있다.
- Universal Approximation Theorem
    
    ![Untitled](%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC%20101%20-%20%E1%84%80%E1%85%B3%E1%86%AB%E1%84%89%E1%85%A1%E1%84%92%E1%85%A1%E1%86%B7%E1%84%89%E1%85%AE%E1%84%85%E1%85%B3%E1%86%AF%20%E1%84%8E%E1%85%A1%E1%86%BD%E1%84%8C%E1%85%A1%2000b35dc3fdb5455dab940988d3c5c878/Untitled%205.png)
    
    ![Untitled](%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC%20101%20-%20%E1%84%80%E1%85%B3%E1%86%AB%E1%84%89%E1%85%A1%E1%84%92%E1%85%A1%E1%86%B7%E1%84%89%E1%85%AE%E1%84%85%E1%85%B3%E1%86%AF%20%E1%84%8E%E1%85%A1%E1%86%BD%E1%84%8C%E1%85%A1%2000b35dc3fdb5455dab940988d3c5c878/Untitled%206.png)
    

### Arbitrary-depth case

- d = input dimension
- D = output dimension
- 너비(뉴런의 개수)를 d+D+2 를 고정하되 깊이(층의 개수)를 얼마나 쌓아야할지는 안 정함.