# ì–¸ì–´ (1) - LLMsì´ë€?

íƒœê·¸: Gemini, LLMs, ì–¸ì–´ëª¨ë¸
No.: 17

## LLM and Prompting

- â­ì‹¤ìŠµ [colab](https://colab.research.google.com/drive/1mrn8UwCRRdtTiOqnGj6Xf4Upo-N-yKoz#scrollTo=sVrm7VyTECQA)
- ë‹¤ë£° ê²ƒ
    - Python SDK for Google Gemini Pro
    - Prompting
    - Retriever Augmented Generation
    - Streamlit
    - Hugging Face

### AI Re-visit

- AI algorithms are just Approximation Functions
    - ëª¨ë“  ê²ƒì€ ë‹¨ì§€ í•¨ìˆ˜ë¥¼ ê·¼ì‚¬í•˜ëŠ” ê²ƒ ë¿ì´ë‹¤!
- ì¸ê°„ì˜ íŒë‹¨ë ¥ì„ ëª¨ì‚¬í•˜ëŠ” ì¸ê³µì§€ëŠ¥
    - classification, object detection, segmentation
    - ììœ¨ì£¼í–‰, ì•ˆë©´ì¸ì‹, ...
    - ì •í™•í•œ í•˜ë‚˜ì˜ ì •ë‹µì„ ë‚´ì•¼ í•¨
- ì¸ê°„ì˜ ì°½ì˜ë ¥ì„ ëª¨ì‚¬í•˜ëŠ” ì¸ê³µì§€ëŠ¥
    - LLM: ChatGPT(OpenAI), Gemini(Google), LLaMA2(Meta), ...
    - Diffusion: DALL-E(OpenAI), Stable Diffusion(Stability AI), ...
    - ì¸ê°„ì´ ìˆ˜ìš©í•  ìˆ˜ ìˆëŠ” ì •ë‹µ, ëª©í‘œë¥¼ ì´ë£¨ê¸° ìœ„í•œ ì–´ë–¤ ì •ë‹µ
        - ê·¸ëŸ¬ë‹¤ ë³´ë‹ˆ í‰ê°€ë¥¼ ì‚¬ëŒì´ í•œë‹¤. (Metrics ì •í•˜ê¸°ê°€ ì–´ë ¤ì›€)

### Large Language Models(LLMs)

- í•„ìš”í•œ ê²ƒ
    1. a method for evaluation
        - human feedback: ë¹„ìŒˆ
        - GPT4 feedback: ë¹„ìš©ì ˆê°ì„ ìœ„í•´ ì–´ì©”ìˆ˜ì—†ì´ ì‚¬ìš©, í•˜ì§€ë§Œ GPT4ë³´ë‹¤ ë›°ì–´ë‚œ ëª¨ë¸ ë§Œë“¤ê¸´ ì–´ë ¤ì›€.
            - ì„œë¡œ ë‹¤ë¥¸ ë°ì´í„°ì…‹ì— ëŒ€í•´ ì—¬ëŸ¬ê°€ì§€ ëª¨ë¸ì„ í•™ìŠµí•˜ê³ , ì´ë¥¼ ensembleí•˜ëŠ” ë°©ë²•ìœ¼ë¡œ ê·¹ë³µí•˜ë ¤ê³  í•¨
    2. candidate models
        - some variations of Transformer: Attention is all you need (2017)
    3. a way to find the parameters:
        - predict the next token
            - train dataset: Wikipedia, Github, News papers, ...
            - real valued differentiable cost function: cross-entropy
            - gradient descent algorithm
            - RLHF (Reinforcement learning from human feedback)
- ë©€í‹°ëª¨ë‹¬?
    
    ![https://i.imgur.com/BgrdePI.png](https://i.imgur.com/BgrdePI.png)
    
- ì–´ë ¤ìš´ ì 
    - ì–‘ì§ˆì˜ train dataset ì–»ê¸°ê°€ ì–´ë µë‹¤.
        - ì‹ ë¬¸ê¸°ì‚¬ -> ì–‘ì§ˆì˜ datasetì´ì§€ë§Œ ìš”ì¦˜ì€ ì¸ê³µì§€ëŠ¥ì— í™œìš©í•˜ëŠ” ê±¸ ê¸ˆì§€í•˜ëŠ” ê²½ìš°ê°€ ë§ìŒ.
        - ë„¤ì´ë²„ í´ë¡œë°”: ë¸”ë¡œê·¸, ì˜í™”í‰ë¡ , ëŒ“ê¸€
        - í•œê¸€ ë°ì´í„° ìì²´ê°€ ì ë‹¤.
    - ìš°ë¦¬ë‚˜ë¼ëŠ” ì •ë³´ ê³µìœ ì— ì¸ìƒ‰í•œ ë©´ì´ ì¡°ê¸ˆ ìˆìŒ.
- **ì–¸ì–´ ëª¨ë¸ì˜ ì›ë¦¬: ë‹¤ìŒì— ì˜¬ ë‹¨ì–´(Token)ì„ ì˜ˆì¸¡**
    - ë‹¨ì ì´ ì •ë§ ë‹¨ì ì¼ê¹Œ? ì˜¤íˆë ¤ ì°½ì˜ë ¥ì„ ë³´ì—¬ì¤„ ìˆ˜ ìˆìŒ.
        
        ![https://i.imgur.com/AEzdBMD.png](https://i.imgur.com/AEzdBMD.png)
        
    - ë‹¨ì  -> ì ì ˆí•œ ì¶”ê°€ ì •ë³´ë¥¼ ì œê³µí•˜ë©´, í•˜ë‚˜ì˜ ì •ë‹µì´ ìƒê¹€

### Prompt Engineering (Prompting)

- ==Prompt Engineering(Prompting)ì´ë€?==
    - ì ì ˆí•œ ì¶”ê°€ì •ë³´ë¥¼ ì œê³µí•˜ì—¬ ì–¸ì–´ëª¨ë¸ì˜ ì°½ì˜ë ¥ì„ ì‚¬ìš©ìê°€ ìˆ˜ìš©í•  ìˆ˜ ìˆëŠ” ë²”ìœ„ë¡œ ì œí•œí•˜ëŠ” ë°©ë²•
        - ì´ê±¸ ë„˜ì–´ì„œë©´ hallucinationì´ë¼ê³  ë¶€ë¥¸ë‹¤.
- Google AI Studio
    - [https://ai.google.dev](https://ai.google.dev/)
    - ê°œì¸ìš© êµ¬ê¸€ idë¡œ ì ‘ì†
    - ê°€ë” ë¶ˆì•ˆì •í•  ë•Œë¥¼ ëŒ€ë¹„í•´ ì„œë¹„ìŠ¤í•˜ëŠ” ê²½ìš°ì—ëŠ” try, except, retryë¥¼ ìœ„í•œ decoratorë¥¼ ì¨ì£¼ë©´ ì¢‹ë‹¤.
- Basic Usage
    - Install SDK
        
        ```python
        pip install -q -U google-generativeai
        
        ```
        
    - Import package
        
        ```python
        import google.generativeai as genai
        
        ```
        
    - API Key
        
        ```python
        genai.configure(api_key=GOOGLE_API_KEY)
        
        ```
        
- Model Parameters
    
    ```python
    cfg = genai.GenerationConfigt(
    	candidate_count = 1 # ë‹µë³€ì˜ ê°œìˆ˜ (ê³ ì •ë˜ì–´ìˆìŒ
    	stop_sequences = None, # í•´ë‹¹ ë¬¸ìì—´ì„ ë§Œë‚˜ë©´ ë¬´ì¡°ê±´ ìƒì„± ì¤‘ë‹¨, ìµœëŒ€ 5ê°œ ë¦¬ìŠ¤íŠ¸ í˜•ì‹ ë¬¸ìì—´
    	max_output_tokens = None, # í† í° ê°œìˆ˜ë¥¼ ëª‡ ê°œë‚˜ ìƒì„±í• ê±´ì§€, ìµœëŒ€ 2048ê°œ (pro ê¸°ì¤€), ë§ì´ ë„ˆë¬´ ê¸¸ì–´ì§€ì§€ ì•Šê²Œ ì¡°ì •í•˜ëŠ” ì—­í• ì„ í•¨
    	temperature = 0.9, # temperature scaling: ì„±ëŠ¥í–¥ìƒì„ ìœ„í•´ classificationì—ì„œ softmax ì§ì „ì— ì–´ë–¤ ê°’ì„ ê³±í•´ì£¼ê±°ë‚˜ ë‚˜ëˆ ì£¼ëŠ” ë°©ë²•
    	top_k = 40
    	top_p = 0.95
    	# ë³´í†µ tempë§Œ ê±´ë“¤ì´ê³  top_k, top_pëŠ” ì˜ ê±´ë“œë¦¬ì§€ëŠ” ì•ŠëŠ”ë‹¤
    
    ```
    
    - Max output tokens
        - Maximum number of tokens that can be generated in the response.
        - A token is approximately four characters. 100 tokensì€ ëŒ€ëµ 60-80ê°œì˜ ë‹¨ì–´ë¡œ ì´ë£¨ì–´ì ¸ìˆë‹¤.
    - â­ Temperature
        - temperature scaling: ì„±ëŠ¥í–¥ìƒì„ ìœ„í•´ classificationì—ì„œ softmax ì§ì „ì— ì–´ë–¤ ê°’ì„ ê³±í•´ì£¼ê±°ë‚˜ ë‚˜ëˆ ì£¼ëŠ” ë°©ë²•
        - temperature: temperature controls the degree of randomness in token selection
            - ì–¸ì–´ëª¨ë¸ì´ ì¶œë ¥ì€ ë‹¤ìŒì— ì˜¬ tokenì˜ í™•ë¥ ê°’ë“¤
                - I ë‹¤ìŒì— be ë™ì‚¬ê°€ ì˜¬ í™•ë¥ : 0.3
                - do ë™ì‚¬ê°€ ì˜¬ í™•ë¥ : 0.2
                - work ë™ì‚¬ê°€ ì˜¬ í™•ë¥ : 0.1 ë“±
                - ìµœì¢…ì ìœ¼ë¡œëŠ” ì´ í™•ë¥ ì— ë§ì¶”ì–´ì„œ samplingì„ í•˜ëŠ” ê²ƒ
            - ì´ëŸ¬í•œ í™•ë¥ ê°’ì„ ë§Œë“¤ê¸° ìœ„í•´ ëª¨ë¸ì˜ ì œì¼ ë°‘ë‹¨ì— ìˆëŠ” ê°’ë“¤ì„ softmaxí•¨ìˆ˜ì— ì§‘ì–´ë„£ì–´ì¤„ ê²ƒ.
                - softmaxì˜ ì—­í• : ê°’ë“¤ì„ ì „ë¶€ 1ë³´ë‹¤ ì‘ê²Œ, ë‹¤ í•©í–ˆì„ ë•Œ 1ë¡œ ë§Œë“¤ì–´ì¤€ë‹¤.
            - ë‚˜ì˜¬ ìˆ˜ ìˆëŠ” ëª¨ë“  TOKENê°’ì— ëŒ€í•´ í™•ë¥ ì„ ê³„ì‚°í•˜ëŠ” ì‹œë„ë¥¼ í•  ê²ƒ -> ë§¤ë²ˆ ë„ˆë¬´ ë§ì€ ê³„ì‚°ì´ í•„ìš”í•´ì§„ë‹¤.ğŸ˜­
                - `Top-K (most probable)`: default = 40, softmaxì— ì§‘ì–´ ë„£ëŠ” ê°’ì„ `logits`ì´ë¼ê³  í•œë‹¤. logitsì€ í•™ìŠµì— ì‚¬ìš©í•œ tokenì˜ ê°œìˆ˜ë§Œí¼ ìˆì—„. ì´ ì¤‘ì— logitsì´ í° ê²ƒ Kê°œë¥¼ ë½‘ëŠ”ë‹¤. -> softmaxì—ì„œ ìˆœì„œê°€ ë°”ë€Œì§€ ì•Šê¸° ë•Œë¬¸ì—, softmaxë¥¼ ê±°ì³ ë‚˜ì˜¨ í™•ë¥ ì´ ì œì¼ ë†’ì€ kê°œì™€ ë™ì¼í•  ê²ƒ
                - ì´ top-K logitsì„ temperatureë¡œ ë‚˜ëˆ„ì–´ì¤€ ë‹¤ìŒ softmaxì— ë„£ëŠ”ë‹¤.
                    - ì‘ì€ ê°’ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë•Œë¬¸ì— ê²°ê³¼ëŠ” ë§¤ìš° ì»¤ì§ˆ ê²ƒ.
                    - í° ê°’ìœ¼ë¡œ ë‚˜ëˆ„ë©´ ê²°ê³¼ëŠ” ë§¤ìš° ì‘ì•„ì§€ê³ , ì´ê±¸ softmaxì— ë„£ìœ¼ë©´ íŒíŒí•´ì§ˆ ê²ƒ
                    - **ì¦‰, ì°½ì˜ë ¥(ë‹µë³€ì´ ë‹¤ì–‘í•˜ë©´ ì¢‹ê² ë‹¤)ì„ í‚¤ìš°ê³  ì‹¶ìœ¼ë©´ temp(ë‚˜ëˆ„ëŠ” ê°’)ì„ í¬ê²Œ, ê·¸ ë°˜ëŒ€ì˜ ê²½ìš°(ê·¸ëŸ´ë“¯í•œ ë‹µë§Œ ë½‘ì•„ë‚´ê³  ì‹¶ë‹¤)ì—ëŠ” tempë¥¼ ì‘ê²Œ ì£¼ë©´ ëœë‹¤.**
                - `Top-P`: default = 0.95, tokens are selected from the most (top-K) to least probable until the sum of their probabilities equals the top-P value. Spcify a lower value for less random responses and a higher value for more random responses.
                    - tokens A = 0.3, B = 0.2, C = 0.1, D= 0.1 ì´ê³  Top-p = 0.5ë©´, A, Bê¹Œì§€ë§Œ ì„ íƒí•˜ê³  C, DëŠ” ì„ íƒ ì•ˆí•¨. í¬ê²Œ ì¤„ìˆ˜ë¡ ì°½ì˜ë ¥ or randomnessì´ ì»¤ì§.
            - ëª¨ë¸ì€ ë‹¤ìŒì— ì˜¬ ê°’ì„ ë™ì‚¬ ì¤‘ì—ì„œ ë½‘ê² ë‹¤!ë¼ê³  ì‚¬ê³ í•˜ëŠ” ê²ƒì´ ì•„ë‹Œ, ë…¸ë˜ë¥¼ ì™¸ìš°ëŠ” ê²ƒì²˜ëŸ¼ ë‹¤ìŒì— ì˜¬ ê°’ì„ ìì—°ìŠ¤ëŸ½ê²Œ! ì™¸ìš´ ê²ƒë“¤ ì¤‘ì—ì„œ ë‚´ë³´ë‚¸ë‹¤. (ì—: ë™í•´ë¬¼ê³¼ -> ë°±ë‘ì‚°ì´, ë…ë„ëŠ” -> ìš°ë¦¬ë•…, ë…ë„ëŠ” -> ì•„ë¦„ë‹µë‹¤ (ì´ê²ƒë„ ë¬¸ì œëŠ” ì—†ìŒ, ë…ë„ëŠ” ìš°ë¦¬ë•…ë§Œ ë‚˜ì˜¤ë©´ ì•ˆ ë˜ê³  ì´ëŸ° ë¬¸ì¥ë„ ë‚˜ì˜¬ í™•ë¥ ì´ ì¡´ì¬í•´ì•¼ í•¨))
- Generate Text from text imputs
    
    ```python
    model = genai.GenerativeModel('gemini-pro')
    response = model.generate_content("ë™í•´ë¬¼ê³¼ ", generation_config=cfg)
    ```
    
- Generate Text from image and text inputs
    - PIL, jpg ë“± ì•„ë¬´ê±°ë‚˜ ë„£ì–´ë„ ì˜ ì²˜ë¦¬í•´ì¤€ë‹¤.
        
        ```python
        model = genai.GenerativeModel('gemini-pro-vision')
        response = model.generate_content(
        	  [
        		  "Transform the following recipt in a markdown table in English."
        		  img  # just put PIL image, bytes code with mime type, etc.
        	  ],
        	  generation_config=cfg
        )
        
        ```
        
- Response format
    
    ```python
    response.candidates
    
    ```
    
    ```python
    [content{
    	parts {
    		text: MODEL RESPONSE TEXT HERE
    	}
    	role: "model"  # model or user
    	}
    finish_reason: STOP or SAFITY # STOPì´ ì •ìƒ, ëª¨ë¸ì´ ë¬¸ì¥ì„ ëë‚´ì„œ STOPì„ ì£¼ê±°ë‚˜ stop_sequencesì— ìˆëŠ” ë‹¨ì–´ë¥¼ ë§Œë‚¬ì„ ë•Œ STOPì„ ì¤€ë‹¤. NETWORK ERROR, INTERNAL ERROR ë“±ë„ ë„£ì„ ìˆ˜ ìˆìŒ. SAFITY: í˜ì˜¤, ì°¨ë³„ ë°œì–¸ ë“±ì„ safety_ratingsë¡œ ê²€ì—´
    index: 0
    saftey_ratings{ # ìƒì„±í•œ ê²ƒì— ëŒ€í•´ì„œë„ ê³„ì† í‰ê°€ë¥¼ í•œë‹¤
    		category: HARM_CATEOGRY_{}
    		probability: {}
    }]
    
    ```
    
- To get text
    
    ```python
    response.candidates[0].content.parts[0].text
    
    ```
    
- Short cut for getting text
    
    ```
    response.text
    # does not work if `stream=True` in `model.generate_content()`
    
    ```
    
- For stream output: ì‚¬ëŒì²˜ëŸ¼ ê¸€ìë¥¼ í•˜ë‚˜ì”© ì°ì–´ì£¼ëŠ” ê²ƒ , ì†ë„ ì°¨ì´ëŠ” ì—†ìŒ
    
    ```python
    for chunk in response:
    	# print(chunk.candidates[0].content.parts[0].text)
    	# shortcut works here! (after loop iteration)
    	print(chunk.text)
    
    ```
    
    - for debugging
        
        ![https://i.imgur.com/tvaDJbh.png](https://i.imgur.com/tvaDJbh.png)
        
- Generate text from image and text inputs (ë©€í‹°ëª¨ë‹¬)
    - (ê²½í—˜ì  tip) tempë¥¼ ì¢€ ì‘ê²Œ ì£¼ëŠ” ê²ƒì´ ì¢‹ë‹¤.
        
        ```python
        !curl -o image.jpg <https://upload.wikimedia.org/wikipedia/commons/0/0b/ReceiptSwiss.jpg> # ì´ë¯¸ì§€ í•˜ë‚˜ ë¶ˆëŸ¬ì˜¤ê¸°
        
        ```
        
        ```python
        import PIL.Image
        img = PIL.Image.open('image.jpg')
        img
        ```
        
        ```python
        cfg = genai.GenerationConfig(
        Â  Â  candidate_count = 1,
        Â  Â  stop_sequences = None,
        Â  Â  max_output_tokens = None,
        Â  Â  temperature = 0.1,  # ì‘ê²Œ
        Â  Â  top_k = 32,
        Â  Â  top_p = 0.5,
        )
        model = genai.GenerativeModel('gemini-pro-vision')
        response = model.generate_content(
        Â  Â  [
        Â  Â  Â  Â  "Transform the following recipt in a markdown table in English.",
        Â  Â  Â  Â  img
        Â  Â  ], generation_config=cfg, stream=True)
        response.resolve()
        to_markdown(response.text)
        
        ```
        

### Embedding generation

- ì°¸ê³  document ([link](https://www.notion.so/angieeee/%5B%3Chttps://ai.google.dev/examples/doc_search_emb?hl=en%3E%5D(%3Chttps://colab.research.google.com/corgiredirector?site=https%3A%2F%2Fai.google.dev%2Fexamples%2Fdoc_search_emb%3Fhl%3Den%3E)))
- retrieval augmentation generationì˜ í•µì‹¬!
    - ëª¨ë“  ë¬¸ì„œë¥¼ ë‹¤ ì¸í’‹ìœ¼ë¡œ ë°›ì•„ì¤„ ìˆ˜ ì—†ìœ¼ë‹ˆ, ì§ˆë¬¸ê³¼ ìœ ì‚¬í•œ ë¬¸ì„œë¥¼ ì„ íƒí•´ì„œ (cosine similarity ì´ìš©) ê°€ì ¸ì˜¨ë‹¤.
- Task types
    - retrieval_query: ì§ˆë¬¸ì„ ë²¡í„°ë¡œ
    - retrieval_document:
        - ì§ˆë¬¸ì— ëŒ€í•œ ë‹µì´ ë“¤ì–´ìˆì„ í™•ë¥ ì´ ë†’ì€ documentë¥¼ ì„ íƒ
            - Consine similarity, dot product ê°’ ë“±ìœ¼ë¡œ ìœ ì‚¬ë„ í‰ê°€
    - semantic_similarity: ì˜ë¯¸ê°€ ë¹„ìŠ·í•œ ê²ƒë¼ë¦¬ ëª¨ìœ¼ëŠ” ìš©ë„ (ì´ê²ƒë„ ë²¡í„°ë¡œ)
    - classification: ë¶„ë¥˜ì— ì‚¬ìš©í•˜ê¸° ìœ„í•œ ë²¡í„°
    - clustering: ë¹„ìŠ·í•œ ê²ƒë¼ë¦¬ ëª¨ìœ¼ê¸° ìœ„í•œ ë²¡í„°
- code
    
    ```python
    import numpy as np
    import pandas as pd
    
    DOCUMENT1 = {
    Â  Â  "title": "Operating the Climate Control System",
    Â  Â  "content": "Your Googlecar has a climate control system that allows you to adjust the temperature and airflow in the car. To operate the climate control system, use the buttons and knobs located on the center console. Â Temperature: The temperature knob controls the temperature inside the car. Turn the knob clockwise to increase the temperature or counterclockwise to decrease the temperature. Airflow: The airflow knob controls the amount of airflow inside the car. Turn the knob clockwise to increase the airflow or counterclockwise to decrease the airflow. Fan speed: The fan speed knob controls the speed of the fan. Turn the knob clockwise to increase the fan speed or counterclockwise to decrease the fan speed. Mode: The mode button allows you to select the desired mode. The available modes are: Auto: The car will automatically adjust the temperature and airflow to maintain a comfortable level. Cool: The car will blow cool air into the car. Heat: The car will blow warm air into the car. Defrost: The car will blow warm air onto the windshield to defrost it."}
    
    DOCUMENT2 = {
    Â  Â  "title": "Touchscreen",
    Â  Â  "content": "Your Googlecar has a large touchscreen display that provides access to a variety of features, including navigation, entertainment, and climate control. To use the touchscreen display, simply touch the desired icon. Â For example, you can touch the \\"Navigation\\" icon to get directions to your destination or touch the \\"Music\\" icon to play your favorite songs."}
    
    DOCUMENT3 = {
    Â  Â  "title": "Shifting Gears",
    Â  Â  "content": "Your Googlecar has an automatic transmission. To shift gears, simply move the shift lever to the desired position. Â Park: This position is used when you are parked. The wheels are locked and the car cannot move. Reverse: This position is used to back up. Neutral: This position is used when you are stopped at a light or in traffic. The car is not in gear and will not move unless you press the gas pedal. Drive: This position is used to drive forward. Low: This position is used for driving in snow or other slippery conditions."}
    
    df = pd.DataFrame([DOCUMENT1, DOCUMENT2, DOCUMENT3])
    df.columns = ['Title', 'Text']
    df
    ```
    
    ```python
    from google.api_core import retry
    
    # Get the embeddings of each text and add to an embeddings column in the dataframe
    
    @retry.Retry(timeout=300.0) Â # ì‹¤íŒ¨í•˜ë©´ 5ë¶„ ë™ì•ˆ ì¬ì‹œë„í•´ë³´ë¼ëŠ” retry decorator
    def embed_fn(title, text): # ë¬¸ì„œì˜ ì œëª©ê³¼ ë‚´ìš©ì„ ë°›ì•„ì„œ ë²¡í„° ë§Œë“œëŠ” í•¨ìˆ˜
    Â  return genai.embed_content(model='models/embedding-001', # ì„ë² ë“œìš© ëª¨ë¸ ì´ë¦„ ì§€ì •
    Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â content=text,
    Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â task_type="retrieval_document",
    Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â title=title)["embedding"]
    
    ```
    
    ```python
    df['Embeddings'] = df.apply(lambda row: embed_fn(row['Title'], row['Text']), axis=1)
    df
    
    ```
    
    ```python
    len(df['Embeddings'].iloc[0]) # ê¸¸ì´ê°€ 768ì¸ ë²¡í„°ë¡œ ë§Œë“¤ì–´ì¤Œ
    
    ```
    
    ```python
    query = "How do you shift gears in the Google car?"
    request = genai.embed_content(model='models/embedding-001',
    Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  content=query,
    Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  task_type="retrieval_query")
    
    ```
    
    ```python
    len(request['embedding'])
    
    ```
    
    ```python
    dot_products = np.dot(np.stack(df['Embeddings']), request['embedding']) # ì„ë² ë”© ì»¬ëŸ¼ì— ìˆëŠ” ê²ƒë“¤ì„ ìŒ“ì•„ì£¼ê³ , requestì„ë² ë”©ì— ìˆëŠ” ê²ƒë“¤ë¡œ dot productí•´ì¤Œ
    idx = np.argmax(dot_products)
    to_markdown('## ' + df.iloc[idx]['Title'] + '\\n\\n' + df.iloc[idx]['Text']) Â # ì§ˆë¬¸ì— ëŒ€í•œ ê°€ì¥ ì—°ê´€ì„±ì´ ë†’ì€ documentì¸ Shifting Gearsë¥¼ ë½‘ì•„ì£¼ì—ˆë‹¤!
    
    ```
    
    ```python
    dot_products # ë’¤ì— ìˆëŠ” ë¬¸ì„œê°€ ê°’ì´ ë” ë†’ë‹¤!
    # output: array([0.66797978, 0.66149501, 0.77680849])
    
    ```
    

### Question and Answering Application

- ìœ„ì— ë°°ìš´ ë‚´ìš©ì„ ì ‘ëª©ì‹œì¼œë³´ì!
    
    ```python
    def find_best_passage(query, dataframe):
    
    Â  """
    Â  Compute the distances between the query and each document in the dataframe
    Â  using the dot product.
    Â  """
    Â  query_embedding = genai.embed_content(model='models/embedding-001',
    Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  content=query,
    Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  task_type="retrieval_query")
    Â  dot_products = np.dot(np.stack(dataframe['Embeddings']), query_embedding["embedding"])
    Â  idx = np.argmax(dot_products)
    Â  return dataframe.iloc[idx]['Text'] # Return text from index with max value
    
    def make_prompt(query, relevant_passage): # ì§ˆë¬¸(query), relevant_passages - ìœ„ì˜ find_best_passageì—ì„œ ë½‘íŒ, ê°€ì¥ ì—°ê´€ì„±ì´ ë†’ì€ documentë¥¼ ê°™ì´ ë„£ì–´ì¤€ë‹¤.
    Â  escaped = relevant_passage.replace("'", "").replace('"', "").replace("\\n", " ")
    Â  prompt = textwrap.dedent("""You are a helpful and informative bot that answers questions using text from the reference passage included below. \\
    Â  Be sure to respond in a complete sentence, being comprehensive, including all relevant background information. \\
    Â  However, you are talking to a non-technical audience, so be sure to break down complicated concepts and \\
    Â  strike a friendly and converstional tone. \\
    Â  If the passage is irrelevant to the answer, you may ignore it.
    Â  QUESTION: '{query}'
    Â  PASSAGE: '{relevant_passage}'
    Â  Â  ANSWER:
    Â  """).format(query=query, relevant_passage=escaped)
    Â  return prompt
    
    ```
    
    ```python
    cfg = genai.GenerationConfig(
    Â  Â  candidate_count = 1,
    Â  Â  stop_sequences = None,
    Â  Â  max_output_tokens = None,
    Â  Â  temperature = 0., Â # ì–¸ì–´/ìƒì„± ëª¨ë¸ì—ì„œ ì‹œì—°í•  ë•Œì—ëŠ” tempë¥¼ 0ìœ¼ë¡œ ë§¤ìš° ë‚®ê²Œ ì£¼ì..
    Â  Â  top_k = 40,
    Â  Â  top_p = 1.,
    )
    
    ```
    
    ```python
    query = "How do you shift gears in the Google car?"
    passage = find_best_passage(query, df)
    prompt = make_prompt(query, passage)
    
    model = genai.GenerativeModel('gemini-pro')
    answer = model.generate_content(prompt, generation_config=cfg)
    to_markdown(answer.text)
    
    ```
    
    ```python
    # promptì—†ì´ ê·¸ëƒ¥ ë¬¼ì–´ë³´ë©´? hallucinationì„ ë³´ì—¬ì¤€ë‹¤..
    answer = model.generate_content(query, generation_config=cfg)
    to_markdown(answer.text)
    
    ```
    
- ì´ë ‡ê²Œ ì ì ˆí•œ ì¶”ê°€ ì •ë³´(prompting)ì„ ì£¼ê³  ë‹µë³€ì„ ì´ëŒì–´ë‚´ëŠ” ê±¸ RAG (Retriever Augmentation Generation)ë¼ê³  í•œë‹¤.
- ì›ë¦¬ë§Œ ì¼ë‹¨ ì‚´í´ë´„
    - ì‹¤ì œë¡œ í° ì‘ì—…ì„ í•´ì•¼í•  ë•Œì—ëŠ” ì—¬ëŸ¬ê°€ì§€ íŒ¨í‚¤ì§€ë¥¼ ì´ìš©í•  ìˆ˜ ìˆë‹¤.

### Multi-turn Conversation (Chat)

- code
    
    ```python
    model = genai.GenerativeModel('gemini-pro')
    chat = model.start_chat(history=[])
    response = chat.send_message("In one sentence, explain how a computer works to a young child.")
    to_markdown(response.text)
    
    ```
    
    ```python
    chat.history # íˆìŠ¤í† ë¦¬ ì°ì–´ë³´ê¸°
    # chat.history[-1]  # ë§ˆì§€ë§‰ ì¤„ë§Œ
    
    ```
    
    ```python
    response = chat.send_message("Okay, how about a more detailed explanation to a high schooler?", stream=True) # stream ê·¸ëƒ¥ ê±¸ì–´ë³¸ ê²ƒ
    for chunk in response:
    Â  print(chunk.text)
    Â  print("_"*80)
    
    ```
    
    ```python
    for message in chat.history:
    Â  display(to_markdown(f'**{message.role}**: {message.parts[0].text}'))
    
    ```
    

`