# Pytorch íŠœí† ë¦¬ì–¼ (3) - A to Z íŒŒì´í”„ë¼ì¸ ì§œë³´ê¸°

íƒœê·¸: HousePrice, Titanic, ë¶„ë¥˜, íšŒê·€
No.: 7

<aside>
ğŸ’¡ íŒŒì¼ì„ ìª¼ê°œê³ , ë¹ ë¥´ê³  ì •í™•í•˜ê²Œ ê²°ê³¼ë¥¼ ë¹„êµí•  ìˆ˜ ìˆë„ë¡ ì„¤ê³„í•˜ì

</aside>

### Process

1) ë°ì´í„° ì „ì²˜ë¦¬

- ë°ì´í„° ë¡œë“œ - pd.DataFrame
- ê²°ì¸¡ê°’, ì´ìƒì¹˜ ì²˜ë¦¬
- EDAë¥¼ í†µí•´ í•„ìš”í•œ featuresë¥¼ ê²°ì •
- ë²¡í„°í™” - numpy

2) ëª¨ë¸ Train

- (K-fold Train & ê²°ê³¼ë¹„êµ)
- í†µìœ¼ë¡œ Train & ê²°ê³¼ í™•ì¸
- ê²°ê³¼ ë° íŒŒë¼ë¯¸í„° ì €ì¥

3) ëª¨ë¸ Test

- ìµœì¢… ì„ íƒ ëª¨ë¸ Test set
- ëª¨ë¸ í‰ê°€
- ì˜ˆì¸¡ ê²°ê³¼ Submission

## (1) Pipeline A to Z - Classification with Titanic Dataset

- data source: [https://www.kaggle.com/competitions/titanic](https://www.kaggle.com/competitions/titanic)
- Random Forest
- Simple NN model

```
### Process
 - X, y ë¶„ë¦¬ëœ ë°ì´í„° ì¤€ë¹„
 - ë°ì´í„°ë¥¼ Pytorch Datasetìœ¼ë¡œ ë³€í™˜ 
 - Dataloaderë¥¼ ì´ìš©í•´ì„œ batch sizeë³„ë¡œ ê°€ì ¸ì˜¤ê¸°
 - Hyperparameters ì„¤ì • (optimizer, lr ë“±)
 - ëª¨ë¸ í•™ìŠµ (model, dataloader, loss function, optimizer)
    - train set í•™ìŠµ
    - í‰ê°€
    - í•™ìŠµí•œ best weights ì €ì¥ -> ìš”ê±° ë”°ë¡œ í•´ë³´ê¸°!
 - test set ê²€ì¦
```

- ë””ë ‰í† ë¦¬ êµ¬ì¡°
    - /data
    - metric.py
    - [nn.py](http://nn.py): models
    - preprocess.py
    - rf.py: random forest
    - train.py: training function
    - utils.py

## (2) Pipeline A to Z - Regression with House Price Dataset

- data source: [https://www.kaggle.com/c/home-data-for-ml-course](https://www.kaggle.com/c/home-data-for-ml-course)
- configì™€ parserë¥¼ ì´ìš©í•´ì„œ ë¹ ë¥¸ ì‹¤í—˜ì„ í•  ìˆ˜ ìˆëŠ” íŒŒì´í”„ë¼ì¸ì„ êµ¬ì¶•í•´ë³¸ë‹¤.
- ë””ë ‰í† ë¦¬ êµ¬ì¡°
    - /data
    - [config.py](http://config.py)
    - [kfold.py](http://kfold.py)
    - nn.py
    - test.py
    - train.py
    - utils.py
    - [validate.py](http://validate.py)
    

## (3) Scratchë¶€í„° íŒŒì´í”„ë¼ì¸ ì§œë³´ê¸°

### ì°¸ê³  ìë£Œ

- [Pytorch Quickstart](https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html)

### ë°°ìš´ ì 

- ë¹ ë¥´ê²Œ í•™ìŠµì´ ë˜ëŠ” ì§€, ì‹¤í—˜ì„ ë¨¼ì € í•´ë³´ê³  ì‹¶ë‹¤ë©´ quickstartì²˜ëŸ¼ ìµœëŒ€í•œ ê°€ë³ê²Œ í•™ìŠµ/í‰ê°€ë¥¼ ì§„í–‰í•´ë³´ì
- Scratchë¶€í„° ì‹œì‘í•˜ëŠ” ì—°ìŠµí•˜ê¸°
    - í•´ë³´ë‹ˆ: ì½”ë“œ ì‘ì„± ì „ì— êµ¬ì¡°ë¥¼ ë¹ ì‚­í•˜ê²Œ ì•Œê³  ìˆì–´ì•¼ ë°”ë¡œë°”ë¡œ ë‚˜ì˜¬ ê²ƒ ê°™ìŒ.
        - ê¸°ì¡´ íŒŒì´í”„ë¼ì¸ì€ cvë„ í•˜ê³ , trianë”°ë¡œ, testë”°ë¡œ ì´ëŸ°ì‹ìœ¼ë¡œ ì§„í–‰í–ˆëŠ”ë°, ë…¸íŠ¸ë¶ì—ì„œ ì§„í–‰í•  ìˆ˜ ìˆëŠ” ê°€ë²¼ìš´ ë²„ì „ì˜ íŒŒì´í”„ë¼ì¸ë„ ì—°ìŠµí•´ë³´ì.
- ì§ˆë¬¸: ê°•ì‚¬ë‹˜ì´ ì´ë²ˆì— ë…¸íŠ¸ë¶ì—ì„œ ì‘ì„±í•˜ì‹  ì½”ë“œë¥¼ ë³´ë‹ˆ, eval()ìª½ì—ì„œ ë”°ë¡œ dataloader ë£¨í”„ë¥¼ í•˜ì§€ ì•Šê³ , ë°–ì—ì„œ next()ë¥¼ í†µí•´ì„œ ë¶ˆëŸ¬ì˜´. ì´í•´í•œ ë‚´ìš© ë§ëŠ”ì§€ ì§ˆë¬¸í•˜ì.
    - input, target = next(iter(tst_dl))  ### testí•¨ìˆ˜ì—ì„œ dataloader ë”°ë¡œ ê°€ì ¸ì˜¤ëŠ” loopë¶€ë¶„ì´ ì—†ì—ˆìŒ. ì—¬ê¸°ì—ì„œ ì²˜ë¦¬í•˜ëŠ” ê²ƒ ê°™ë‹¤.
        # iter()ëŠ” ë°ì´í„° ë¡œë”ë¥¼ iterable ê°ì²´ë¡œ ë§Œë“¤ì–´ì¤Œ. ì´ë•Œ, testì…‹ì´ê¸° ë•Œë¬¸ì— ë”°ë¡œ batchë¡œ ìª¼ê°œì„œ ê°€ì ¸ì˜¤ëŠ” ê²Œ ì•„ë‹ˆë¼, 
        # ë°ì´í„°ì…‹ ì „ì²´ë¥¼ í†µìœ¼ë¡œ ê°€ì ¸ì˜´ (ê·¸ë˜ì•¼ ì „ì²´ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì—ì„œ ëª¨ë¸ì´ ì–¼ë§ˆë‚˜ ì˜ ìˆ˜í–‰ë˜ëŠ”ì§€ë¥¼ ë¹ ë¥´ê²Œ í‰ê°€í•  ìˆ˜ ìˆìŒ)
        # next()ëŠ” iterable ê°ì²´ì—ì„œ ë‹¤ìŒ ìš”ì†Œë¥¼ ê°€ì ¸ì˜¤ëŠ” ê²ƒ. ì—¬ê¸°ì—ì„œëŠ” í†µìœ¼ë¡œ ê°€ì ¸ì˜¨ ë°ì´í„°ë¥¼ í•œ ë²ˆì— ëª¨ë¸ì— ì „ë‹¬í•´ì„œ í‰ê°€ ìˆ˜í–‰

### êµ¬ì¡°

```
# ------------------ ë°ì´í„° ë¡œë“œ --------------- #
# Dataset ë¶ˆëŸ¬ì˜¤ê¸° (train, test)
# ì›í•˜ëŠ” Batch ì‚¬ì´ë“œ ë³„ë¡œ ë¶ˆëŸ¬ì˜¤ê¸° ìœ„í•´ Dataloaderì— ë„£ê¸°
# ------------------ ëª¨ë¸ ì„¤ê³„ ----------------- #
# nn ëª¨ë¸ ì •ì˜ (init, forward)
# model.to(device)
# ------------------ ëª¨ë¸ í•™ìŠµ, ìµœì í™” ----------- #
# loss function
# optimizer
# train í•¨ìˆ˜ ì •ì˜(dataloader, model, loss, optimizer)
	# batch sizeë³„ë¡œ train loop ëŒê¸°
		# compute prediction error
		# backpropagation
# test ì…‹ì—ì„œë„ í•™ìŠµ ì˜ ë˜ëŠ”ì§€ í™•ì¸í•˜ê¸°(dataloader, model, loss)
	# batch sizeë¥¼ í…ŒìŠ¤íŠ¸ì…‹ í†µìœ¼ë¡œ ì„¤ì •í•´ì„œ ëŒë¦¬ê¸°
		# with torch.inference (optimize í•˜ì§€ ì•Šê³  í™•ì¸: no gradient)
			# loss or metric ê³„ì‚°
# í•™ìŠµ loop ëŒê¸° (epoch ì„¤ì •: epochë§ˆë‹¤ ëª¨ë¸ì€ íŒŒë¼ë¯¸í„°ë¥¼ í•™ìŠµí•˜ê³  ì—…ë°ì´íŠ¸í•¨)
	# ì´ë•Œ train set, test set ê°ê° í•¨ìˆ˜ ì‚¬ìš©í•´ì„œ ë™ì‹œì— í™•ì¸í•  ìˆ˜ ìˆìŒ
# ì›í•˜ëŠ” ëª¨ë¸ì„ torch.save
# ë‚˜ì¤‘ì— í•´ë‹¹ ëª¨ë¸ì„ loadí•´ì„œ predictionì„ ì–»ì„ ìˆ˜ ìˆìŒ.
```

### Task 1 ë‹¤ì‹œ í•´ë³´ê¸°

- ë©”ì¸ ë¶€ë¶„ë§Œ

```python
# train í•¨ìˆ˜ for one epoch
def train(dataloader, model, loss_function, optimizer):
    model.train() # train ëª¨ë“œ ì§„ì…
    total_loss = 0. # initialize 
    for input,target in dataloader: # dataloaderì—ì„œ ì•Œì•„ì„œ batchë³„ë¡œ êº¼ë‚´ì˜¬ê±°ì„
        input, target = input.to(device), target.to(device)
        # compute prediction error
        pred = model(input)
        loss = loss_function(pred, target)
        # backpropagation
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        # loss ì§ì ‘ ê³„ì‚°
        total_loss += loss.item()*len(pred)  # loss.item() loss for every batch 
    return total_loss/len(dataloader.dataset) # í‰ê·  lossê°’

# test ì…‹ì— ë„£ì–´ì„œ ë°”ë¡œë°”ë¡œ metric í™•ì¸í•´ë³¼ ìš©ë„, testì…‹ì—ì„œë„ í•™ìŠµ ì˜ ë˜ëŠ”ì§€ í™•ì¸ë¶€í„° í•´ì•¼í•˜ë‹ˆê¹Œ
def test(model, input, target, loss_function, metric, return_pred:bool=False):  # return_predì´ Trueì¼ë•Œë§Œ, predictionê°’ return
    model.eval() # eval ëª¨ë“œ ì§„ì…
    with torch.inference_mode():   ################ 
        # TODO ì§ˆë¬¸: # https://colab.research.google.com/drive/1uO3Ep1GNB0GMEuXnzy1eqvX9ezovHFnm#scrollTo=VgDZYq-uFhy-
        # compute prediction error
        pred = model(input.to(device))
        tst_loss = loss_function(pred, target.to(device)).item()
        # metricë„ ë„£ì–´ì„œ í‰ê°€í•˜ì
        tst_mae = metric(pred, target.to(device)).item()
        # df_metric = pd.DataFrame([loss, mae], index = ["tst_loss: MSE", "tst_metric: MAE"])
        pred_np = pred.numpy()  ### prediction ê°’ ë‚´ë³´ë‚´ê¸° ìœ„í•¨
    if return_pred:
        return tst_loss, tst_mae, pred_np
    return tst_loss, tst_mae
```

```python
from tqdm.auto import trange # for progress bar visualization

model = ANN(input_dim = window_size, output_dim= prediction_size, hidden_dim=128, activation=F.relu)
model.to(device)

loss_function = F.mse_loss
optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)
metric = F.l1_loss # mae

epochs = 2000
pbar = trange(epochs)
for i in pbar:
    trn_mse = train(dataloader=trn_dl, model=model, loss_function=loss_function, optimizer=optimizer)
    input, target = next(iter(tst_dl))  ### testí•¨ìˆ˜ì—ì„œ dataloader ë”°ë¡œ ê°€ì ¸ì˜¤ëŠ” loopë¶€ë¶„ì´ ì—†ì—ˆìŒ. ì—¬ê¸°ì—ì„œ ì²˜ë¦¬í•˜ëŠ” ê²ƒ ê°™ë‹¤.
    # iter()ëŠ” ë°ì´í„° ë¡œë”ë¥¼ iterable ê°ì²´ë¡œ ë§Œë“¤ì–´ì¤Œ. ì´ë•Œ, testì…‹ì´ê¸° ë•Œë¬¸ì— ë”°ë¡œ batchë¡œ ìª¼ê°œì„œ ê°€ì ¸ì˜¤ëŠ” ê²Œ ì•„ë‹ˆë¼, 
    # ë°ì´í„°ì…‹ ì „ì²´ë¥¼ í†µìœ¼ë¡œ ê°€ì ¸ì˜´ (ê·¸ë˜ì•¼ ì „ì²´ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì—ì„œ ëª¨ë¸ì´ ì–¼ë§ˆë‚˜ ì˜ ìˆ˜í–‰ë˜ëŠ”ì§€ë¥¼ ë¹ ë¥´ê²Œ í‰ê°€í•  ìˆ˜ ìˆìŒ)
    # next()ëŠ” iterable ê°ì²´ì—ì„œ ë‹¤ìŒ ìš”ì†Œë¥¼ ê°€ì ¸ì˜¤ëŠ” ê²ƒ. ì—¬ê¸°ì—ì„œëŠ” í†µìœ¼ë¡œ ê°€ì ¸ì˜¨ ë°ì´í„°ë¥¼ í•œ ë²ˆì— ëª¨ë¸ì— ì „ë‹¬í•´ì„œ í‰ê°€ ìˆ˜í–‰
    tst_loss, tst_mae = test(model = model, input=input, target=target, loss_function = loss_function, metric = metric)
    
    pbar.set_postfix({"trn_mse":trn_mse, "tst_mse": tst_loss, "tst_mae": tst_mae})
```